Evaluation and comparisons between the three different models were also the occasion for us to see how choosing and preparing datasets have essential impact on the end results. 

We report the performance of our different models in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney
(2003). These three performance statistics are defined as
recall = |A n S|/|S| 
precision = |A n P|/|A| and
AER = 1 - (|A n S| + |A n P|)/(|A| + |S|)
where S denotes the annotated set of sure alignments, P denotes the annotated set of possible alignments, and A denotes the set of alignments produced by each models under test.

We take AER as our primary evaluation metric. And what was interesting to see considering the very small extend of our unlabeled bilingual corpus was how, as we chose to automatically generate labels from the alignments generated by all models by taking only the maximum alignment for the S sure alignment set and the k biggest alignments for the P possible alignments sets, varying k had impact on the AER of the three models.
[see image from Matlab]
This is an illustration of how independantly of the models in themselves the preparation, labelling and evaluation processes can have interact with a model's property.