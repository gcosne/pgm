%==============================================================================
%== template for LATEX poster =================================================
%==============================================================================
%
%--A0 beamer slide-------------------------------------------------------------
\documentclass[final]{beamer}
\usepackage{etex}
\usepackage[orientation=portrait,size=a0,
            scale=1.25         % font scale factor
           ]{beamerposter}
           
\geometry{
  hmargin=2.5cm, % little modification of margins
}


% MY PACKAGES
\usepackage[export]{adjustbox}


%
\usepackage[utf8]{inputenc}

\linespread{1.15}
%
%==The poster style============================================================
\usetheme{sharelatex}

%==Title, date and authors of the poster=======================================
\title
[Probabilistic Graphical Models Poster Session, Wed 4 Jan 2017] % Conference
{ % Poster title
Alignment models for statistical translation
}

\author{ % Authors
Thomas Debarre, Matthieu Kirchmeyer, Sylvain Truong, Nicolas Zhang
}
%\institute{}
\institute
%[Very Large University] % General University
{\mbox{}
%\inst{1} Very Large University, Neverland\\[0.3ex]
%\inst{2} Other University, Neverland\\[0.3ex]
%\inst{3} Yet Another University, Neverland
}
\date{\today}



\begin{document}
\begin{frame}[t]
%==============================================================================
\begin{multicols}{3}
%==============================================================================
%==The poster content==========================================================
%==============================================================================

\section{Introduction}

This project explores three methods to solve the problem of word alignments for a bilingual corpus from the conventional mixtures models (IBM1, IBM2) to a first-order HMM model developped in \cite{vogel}. 

The goal is to translate a text given in a language (French in all that follows) to another language (English) taking into account one-to-many alignments or null words; the quality of the translation relies on the quality of the word alignments.

We evaluated our algorithms on a small bilingual French-English dataset $\mathcal{D} = \{ (\mathbf{f}^{(1)},\mathbf{e}^{(1)}), \dots , (\mathbf{f}^{(N)},\mathbf{e}^{(N)})\}$ which consists of $N = 22$ French sentences composed of 4 to 10 words with their translated English equivalents. 

\section{Statistical translation models}


\structure{Your text with scientific results or something...} 

(\mathbf{f}, \mathbf{a} \vert \mathbf{e}) = p(I \vert J) \prod_{i=1}^I p(a_i \vert i, J, I) \cdot p(f_i \vert e_{a_i})
\end{equation}


\subsection{IBM models}
The IBM alignment models for statistical machine translation were introduced by IBM more than 20 years ago \cite{brown}. They are increasingly complex, starting from the basic IBM1 model to the more complex IBM6 model. In this project, we implemented the 2 first models, IBM1 and IBM2. Both these models are based on the decomposition of the joint probability of a French sentence $\mathbf{f}$ of length $I$ and its alignment $\mathbf{a}$ conditioned on the English sentence $\mathbf{e}$ of length $J$:
\begin{equation}
p(\mathbf{f}, \mathbf{a} \vert \mathbf{e}) = p(I \vert J) \prod_{i=1}^I p(a_i \vert i, J, I) \cdot p(f_i \vert e_{a_i})
\end{equation}
The alignment variables $\mathbf{a} = (a_1, \hdots, a_I)$ maps a French word $f_i$ in $\mathbf{f}$ to an English word $e_{a_i}$ in $\mathbf{e}$. We have $a_i \in [0, J]$, where $a_i = 0$ is the \textit{null alignment} ($f_i$ does not align to any English word in $\mathbf{e}$).

 for any pair of French and English words $(f,e)$ which are part of the training dataset, the conditional probability $p(f \vert e)$. A French sentence $\mathbf{f}$ of length $I$ is represented by a string of words $\langle f_1, \ldots, f_I \rangle$. Similarly, we represent an English sentence $\mathbf{e}$ of length $J$ by a string of words $\langle e_1, \ldots, e_J \rangle$. We assume that each French word is aligned to a single English word, and we call $a_i \in [0, J]$ the alignment variable ($f_i$ is aligned with $e_{a_i}$ if $a_i > 0$ and is unaligned if $a_i = 0$). In these models, the joint probability of the French sentence and its alignment conditioned on the corresponding English sentence is:
\begin{equation}
p(\mathbf{f}, \mathbf{a} \vert \mathbf{e}) = p(I \vert J) \prod_{i=1}^I p(a_i \vert i, J, I) \cdot p(f_i \vert e_{a_i})
\end{equation}
Both these models assume that for a given sentence pair $(\mathbf{f}, \mathbf{e})$ ($\mathbf{e}$ being the English translation of the French sentence $\mathbf{f}$), the probability of

\begin{equation}
p(\mathbf{f}, \mathbf{a} \vert \mathbf{e}) = p(I \vert J) \prod_{i=1}^I p(a_i \vert i, J, I) \cdot p(f_i \vert e_{a_i})
\end{equation}

\subsubsection{IBM1}
The IBM1 model 

\subsubsection{IBM2}

\vskip1ex
\begin{table}
\centering
\caption{This is a table with scientific results.}
\begin{tabular}{ccccc}
\hline\hline
1 & 2 & 3 & 4 & 5\\
\hline
aaa & bbb & ccc & ddd & eee\\
aaaa & bbbb & cccc & dddd & eeee\\
aaaaa & bbbbb & ccccc & ddddd & eeeee\\
aaaaaa & bbbbbb & cccccc & dddddd & eeeeee\\
1.000 & 2.000 & 3.000 & 4.000 & 5.000\\
\hline\hline
\end{tabular}
\end{table}
\vskip2ex

Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 


\subsection{HMM-based alignment models}

The HMM-based alignment models were introduced by \textit{Vogel and al.} and aim at modeling the joint probability of a French sentence $\textbf{f}$, of length $J$ and an alignment $\textbf{a}\in \{1,\dots,I\}^J$, given a English sentence $\textbf{e}$ of length $I$.
Without, loss of generality, this probability can be expressed as (chain-rule and independance of $\textbf{f}$ and $\textbf{a}$):
\begin{align*}
Pr(\textbf{f},\textbf{a}|\textbf{e}) &= Pr(f_0,a_0|\textbf{e}) \prod_{j=1}^J Pr(f_j,a_j|f_{1}^{j-1},a_{1}^{j-1},\textbf{e})\\
&= Pr(f_0|\textbf{e}) \cdot Pr(a_0|\textbf{e})\\
&\cdot \prod_{j=1}^J Pr(f_j|f_{1}^{j-1},a_{1}^{j-1},\textbf{e}) Pr(a_j|f_{1}^{j-1},a_{1}^{j-1},\textbf{e})
\end{align*}

The HMM model reduced the generality of the previous formula by assuming first-order dependance of alignments and translation probability of French word at position $j$ to be only dependent on the English word at position $a_j$, thus yielding the HMM model:

\begin{align*}
Pr(\textbf{f},\textbf{a}|\textbf{e}) &= p(f_0|e_{a_0}) p(a_0)\\
&\cdot \prod_{j=1}^J p(f_j|e_{a_{j}}) \cdot p(a_j|a_{j-1},I)
\end{align*}

\vskip1ex
\begin{figure}
\centering
%\includegraphics[width=0.99\columnwidth]{logo.png}
\adjincludegraphics[width=0.99\columnwidth,trim={{0.03\width} {0.6\height} {0.02\width} 0},clip]{figures/hmm.pdf}
\caption{The HMM-based alignment model}
\label{fig:hmm}
\end{figure}
\vskip2ex

Figure~\ref{fig:hmm} presents the graphical model associated with HMM-based alignement models. What can be noticed is that, in this context, each sentence pair represents a Hidden Markov Model which parameters are shared with all other sentences of the corpus.
The \textbf{parameters for the HMM model} are
\begin{align*}
\Theta &= \lbrace\\
& p(i) &\mbox{   the initial alignment probabilities ($p(a_0)$)}\\ 
& p(i|i',I) &\mbox{   the transition matrix}\\
& p(f|e) &\mbox{   the emission (translation) probabilities}\rbrace
\end{align*}

A further assumption from \textit{Vogel and al.} is that alignment transition probabilities \textbf{only depend on relative positions}: $p(i|i',I) = p(i'-i|I)$

Just as for the IBM models, the observations are the sequences of French words in French sentences, while the alignments (and thus the corresponding English words) act as hidden variables. A way to solve this problem is to use an \textbf{EM principle}.
The \textbf{Expectation step} consists in computing alignment unary and binary probabilities in each sentence pair $(\textbf{f},\textbf{e})$. 

\begin{align*}
\gamma_i(j) &= p(a_j=i|\textbf{f})\\
\xi_{k,l}(j) &= p(a_j=k,a_{j+1}=l|\textbf{f})
\end{align*}

This can be performed using the Forward-Backward algorithm.
The \textbf{Maximisation step} consists in re-estimating the model's parameters according to the previously computed probabilities.

The \textbf{Viterbi} algorithm is then used to retrieve the most likely alignements in the corpus, according to the computed transition and emission probabilities.

\section{Results and discussions}

\subsection{The experimental setup}

Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something...

\subsubsection{Test data}
 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 

\subsubsection{Evaluation method}
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 



\subsubsection{Example results}
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 
Your text with scientific results or something... 

\section{Summary and conclusions}




%==============================================================================
%==End of content==============================================================
%==============================================================================

%--References------------------------------------------------------------------

\subsection{References}

\begin{thebibliography}{99}

\bibitem{brown} P. Brown et al., The Mathematics of Machine Translation: Parameter Estimation, 1993

\bibitem{vogel} S. Vogel, H. Ney, and C. Tillmann, HMM-based word alignment in statistical translation, 1996

\bibitem{web} \url{http://www.google.pl}

\end{thebibliography}
%--End of references-----------------------------------------------------------

\end{multicols}

%==============================================================================
\end{frame}
\end{document}